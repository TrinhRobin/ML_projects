{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56eae889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#Model \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "#NLP\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#import gensim\n",
    "from gensim.models import Word2Vec\n",
    "#Text processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c557ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pc-02\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\pc-02\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pc-02\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pc-02\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fca3d863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                    text  label\n",
      "0     Why did I only receive a partial amount of wha...      3\n",
      "1                          my atm transaction was wrong      3\n",
      "2                              why did i only get 20.00      3\n",
      "3     Why is my cash request and the amount I receiv...      3\n",
      "4     I tried to withdraw $100, all it gave me was $20.      3\n",
      "...                                                 ...    ...\n",
      "2164  Was charged an ATM fee despite it being a smal...      4\n",
      "2165             I had a wrong fee charged at this ATM.      4\n",
      "2166  When I got cash, I'm pretty sure there was a g...      4\n",
      "2167  My account got charged for taking out cash, wh...      4\n",
      "2168                Why is there a fee for withdrawing?      4\n",
      "\n",
      "[2169 rows x 2 columns]>\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "0    227\n",
      "1    222\n",
      "2    221\n",
      "3    220\n",
      "4    217\n",
      "5    215\n",
      "6    213\n",
      "7    212\n",
      "8    211\n",
      "9    211\n",
      "Name: label, dtype: int64\n",
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc-02\\OneDrive\\Documents\\Anaconda\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_data(csv_file,column_name):\n",
    "    dataframe = pd.read_csv(csv_file)\n",
    "    print(dataframe.head)\n",
    "    print(dataframe.isnull().sum())\n",
    "    category = dataframe[column_name].value_counts()\n",
    "    print(category)\n",
    "    print(sns.barplot(category.index,category))\n",
    "    return(dataframe)\n",
    "data_request = load_data('ds_task_dataset.csv','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0fa6372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing requests\n",
    "#set all letters to lower ones\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text=text.strip()\n",
    "    return text\n",
    "#remoce usual/non relevant words\n",
    "def pertinence(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english') ]\n",
    "    return ' '.join(a)\n",
    "#combine these 2 functions\n",
    "def preprocess_v2(string):\n",
    "    return pertinence(preprocess(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0a69a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>pertinent_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why did I only receive a partial amount of wha...</td>\n",
       "      <td>3</td>\n",
       "      <td>receive partial amount tried withdraw?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my atm transaction was wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>atm transaction wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why did i only get 20.00</td>\n",
       "      <td>3</td>\n",
       "      <td>get 20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why is my cash request and the amount I receiv...</td>\n",
       "      <td>3</td>\n",
       "      <td>cash request amount received different?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I tried to withdraw $100, all it gave me was $20.</td>\n",
       "      <td>3</td>\n",
       "      <td>tried withdraw $100, gave $20.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Why did I only receive a partial amount of wha...      3   \n",
       "1                       my atm transaction was wrong      3   \n",
       "2                           why did i only get 20.00      3   \n",
       "3  Why is my cash request and the amount I receiv...      3   \n",
       "4  I tried to withdraw $100, all it gave me was $20.      3   \n",
       "\n",
       "                            pertinent_text  \n",
       "0   receive partial amount tried withdraw?  \n",
       "1                    atm transaction wrong  \n",
       "2                                get 20.00  \n",
       "3  cash request amount received different?  \n",
       "4           tried withdraw $100, gave $20.  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing data by add a new column : pertinent text\n",
    "data_request['pertinent_text'] = data_request['text'].apply(lambda x: preprocess_v2(x))\n",
    "data_request.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acacb31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a class of  World to Vector model\n",
    "# word2vec algorithm uses a neural network model to learn word associations from a large corpus of text\n",
    "#word2vec represents each distinct word with a particular list of numbers called a vector.\n",
    "#use the cosine similarity between the vectors to compute the level of semantic similarity of the associated words\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "        \n",
    "        ## comply with scikit-learn transformer requirement\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    #Compute average word vector for a single doc/sentence.\n",
    "    #embedding of all words are averaged, and thus we get a 1D vector of features corresponding to each tweet. \n",
    " #   ( the overall average will be a good representation of the tweet.)\n",
    "    #This data format is what typical machine learning models expect\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    # empty words\n",
    "                    # If a text is empty, return a vector of zeros.\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "\n",
    "#Train/test a logistic regression on the dataset\n",
    "def create_model_bag_words(dataset,column_input=\"pertinent_text\",column_predict=\"label\",test_size_value=0.2):\n",
    "    #create a test and train set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset[column_input],\n",
    "                                                        dataset[column_predict],\n",
    "                                                        test_size=test_size_value,shuffle=True)\n",
    "    \n",
    "    #categorization model 1 : Bag-of-Words\n",
    "    #Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "    T_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "    X_train_vec_T = T_vectorizer.fit_transform(X_train) \n",
    "    X_test_vec_T = T_vectorizer.transform(X_test)\n",
    "    \n",
    "    #Training classification model using Logistic Regression(tf-idf)\n",
    "    logistic_regression_model=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "    logistic_regression_model.fit(X_train_vec_T, y_train)  \n",
    "    #Predicting label value for test dataset\n",
    "    y_predict =  logistic_regression_model.predict(X_test_vec_T)\n",
    "    y_prob =  logistic_regression_model.predict_proba(X_test_vec_T)[:,1]\n",
    "    print(classification_report(y_test,y_predict))\n",
    "    print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    "    return( [ T_vectorizer,logistic_regression_model ])\n",
    "\n",
    "#Train/test a classification model World2vec on the dataset\n",
    "def create_model_World_2_Vec(dataset,column_input=\"pertinent_text\",column_predict=\"label\",test_size_value=0.2):\n",
    "     #create a test and train set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset[column_input],dataset[column_predict],\n",
    "                                                        test_size=test_size_value,shuffle=True)\n",
    "    #vectorize the input values/queries\n",
    "    X_train_vec= [nltk.word_tokenize(i) for i in X_train]  \n",
    "    X_test_vec = [nltk.word_tokenize(i) for i in X_test]\n",
    "   \n",
    "    #vectorize the input column\n",
    "    dataset[column_input+'_vec']=[nltk.word_tokenize(i) for i in data_request[column_input]]\n",
    "    #classification_model\n",
    "    classification_model = Word2Vec(dataset[column_input+'_vec'],min_count=1) \n",
    "    #world to vector\n",
    "    w2v = dict(zip(classification_model.wv.index_to_key, classification_model.wv.vectors))\n",
    "    modelw = MeanEmbeddingVectorizer(w2v)\n",
    "    # converting requests to numerical data with Word2Vec\n",
    "    X_train_vec_w2v = modelw.transform(X_train_vec)\n",
    "    X_test_vec_w2v = modelw.transform(X_test_vec)\n",
    "    \n",
    "    #Training classification model using Logistic Regression and World_2_Vector\n",
    "\n",
    "    logistic_regression_w2v=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "    logistic_regression_w2v.fit(X_train_vec_w2v, y_train)  \n",
    "    #Predicting label value for test dataset\n",
    "    y_predict = logistic_regression_w2v.predict(X_test_vec_w2v)\n",
    "    y_prob = logistic_regression_w2v.predict_proba(X_test_vec_w2v)[:,1]\n",
    "    print(classification_report(y_test,y_predict))\n",
    "    print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    "    return([modelw,logistic_regression_w2v])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ee7846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        45\n",
      "           1       1.00      0.94      0.97        48\n",
      "           2       0.93      0.97      0.95        38\n",
      "           3       0.98      0.87      0.92        47\n",
      "           4       0.88      0.93      0.90        41\n",
      "           5       1.00      0.98      0.99        54\n",
      "           6       0.87      0.98      0.92        42\n",
      "           7       0.95      0.97      0.96        38\n",
      "           8       0.96      0.93      0.95        46\n",
      "           9       0.94      0.94      0.94        35\n",
      "\n",
      "    accuracy                           0.94       434\n",
      "   macro avg       0.94      0.95      0.94       434\n",
      "weighted avg       0.95      0.94      0.94       434\n",
      "\n",
      "Confusion Matrix: [[42  0  0  0  2  0  0  1  0  0]\n",
      " [ 0 45  1  0  1  0  0  1  0  0]\n",
      " [ 0  0 37  0  0  0  0  0  0  1]\n",
      " [ 0  0  0 41  2  0  4  0  0  0]\n",
      " [ 1  0  0  0 38  0  2  0  0  0]\n",
      " [ 1  0  0  0  0 53  0  0  0  0]\n",
      " [ 0  0  0  1  0  0 41  0  0  0]\n",
      " [ 1  0  0  0  0  0  0 37  0  0]\n",
      " [ 0  0  2  0  0  0  0  0 43  1]\n",
      " [ 0  0  0  0  0  0  0  0  2 33]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TfidfVectorizer(), LogisticRegression(C=10, solver='liblinear')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_model_bag_words(data_request,column_input=\"pertinent_text\",column_predict=\"label\",test_size_value=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19458e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.89      0.43        35\n",
      "           1       0.68      0.89      0.77        46\n",
      "           2       0.51      0.84      0.63        44\n",
      "           3       0.89      0.75      0.82        56\n",
      "           4       0.86      0.12      0.21        50\n",
      "           5       0.69      0.50      0.58        40\n",
      "           6       0.65      0.55      0.59        40\n",
      "           7       0.18      0.08      0.11        38\n",
      "           8       0.50      0.27      0.35        44\n",
      "           9       0.57      0.49      0.53        41\n",
      "\n",
      "    accuracy                           0.54       434\n",
      "   macro avg       0.58      0.54      0.50       434\n",
      "weighted avg       0.60      0.54      0.51       434\n",
      "\n",
      "Confusion Matrix: [[31  0  0  0  0  2  0  2  0  0]\n",
      " [ 0 41  1  0  0  3  0  1  0  0]\n",
      " [ 0  2 37  0  0  0  0  1  3  1]\n",
      " [ 0  2  4 42  0  0  6  2  0  0]\n",
      " [30  0  4  1  6  2  4  3  0  0]\n",
      " [ 9  8  2  0  0 20  1  0  0  0]\n",
      " [10  1  0  4  1  0 22  2  0  0]\n",
      " [28  1  1  0  0  2  0  3  1  2]\n",
      " [ 0  5 12  0  0  0  1  2 12 12]\n",
      " [ 0  0 12  0  0  0  0  1  8 20]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<__main__.MeanEmbeddingVectorizer at 0x13a74dbb820>,\n",
       " LogisticRegression(C=10, solver='liblinear')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_model_World_2_Vec(data_request,column_input=\"pertinent_text\",column_predict=\"label\",test_size_value=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c2223d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PIPELINE = CODE PRODUCTION READY\n",
    "\n",
    "##Creating a class with fit and transform method\n",
    "class preprocessing():\n",
    "    def __init__(self):\n",
    "        self.n =0\n",
    "    def fit(self,text,label):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "         return np.array([\n",
    "          ' '.join([i for i in word.lower().strip().split() if i not in stopwords.words(\"english\") ])\n",
    "            for word in X\n",
    "        ])\n",
    "\n",
    "class Chatbot():\n",
    "    def __init__(self,pipeline,categories):\n",
    "        self.pipeline = pipeline\n",
    "        self.services = categories\n",
    "    def fit(self,X,y):\n",
    "        self.pipeline.fit(X, y)\n",
    "    def predict(self,X):\n",
    "        return self.pipeline.predict(X)\n",
    "    def discuss(self):\n",
    "        discussion = True\n",
    "        while discussion :\n",
    "            user_input = input(\"Write a sentence :\")\n",
    "            list_words =[ word.lower() for word in user_input.split(' ')] \n",
    "            if \"bye\" in list_words or \"goodbye\" in list_words :\n",
    "               # print(list_words)\n",
    "                print('Goodbye ! I am glad I could help you, Have a Good day !')\n",
    "                discussion = False\n",
    "            else :\n",
    "                service_user =self.pipeline.predict([user_input])\n",
    "                print(f\"Hello, my name is Dumb_Chatbot, the service is {self.services[int(service_user )-1]}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d7e1a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub = data_request.sample(100)\n",
    "categories_list = [\"direct payment\",\"deposit\",\"ATM\",\"Cash\",\"Charging fees\",\n",
    " \"transaction not approved\",\"transfer fee\",\"Transaction not complete\",\"Transfer not shown in balance\"]\n",
    "\n",
    "\n",
    "c = Chatbot(Pipeline([('Pretransfo',preprocessing()),('Vectorizer',TfidfVectorizer(use_idf=True)), \n",
    "                      ('clf',LogisticRegression(C=10, solver='liblinear'))]),\n",
    "   categories_list)\n",
    "c.fit(data_sub[\"text\"], data_sub[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84c88163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.predict(['my atm transaction was wrong'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.discuss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "622bcb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_request['text_vec']=[nltk.word_tokenize(i) for i in data_request[\"text\"]]\n",
    "data_request['pertinent_text_vec']=[nltk.word_tokenize(i) for i in data_request[\"pertinent_text\"]]\n",
    "w2vec = Word2Vec(data_request['pertinent_text_vec'],min_count=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b19b5903",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c = Chatbot(Pipeline([('Pretransfo',preprocessing()),\n",
    "                      ('Vectorizer',MeanEmbeddingVectorizer(\n",
    "                         dict(zip(w2vec.wv.index_to_key, w2vec.wv.vectors)) )), \n",
    "                      ('clf',LogisticRegression(C=10, solver='liblinear'))]),\n",
    "   categories_list)\n",
    "c.fit(data_sub[\"text\"], data_sub[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3388e2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.predict(['my atm transaction was wrong'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
